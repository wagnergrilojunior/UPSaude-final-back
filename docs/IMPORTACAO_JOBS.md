# Processamento assÃ­ncrono de importaÃ§Ãµes (Import Jobs)

Este documento descreve **como funciona** (visÃ£o tÃ©cnica e nÃ£o-tÃ©cnica) o pipeline de importaÃ§Ã£o de arquivos grandes (SIA-PA, SIGTAP, CID10) usando:

- Upload via API (Multipart)
- Armazenamento no **Supabase Storage**
- Enfileiramento via **tabela `public.import_job`**
- Processamento em **background** (Scheduler + Executor dedicado)
- Acompanhamento de progresso pelo frontend via polling

---

## VisÃ£o nÃ£o-tÃ©cnica (para operaÃ§Ã£o/usuÃ¡rio)

- **VocÃª envia um arquivo** (ou, no SIGTAP, o arquivo + layout).
- O sistema **salva o arquivo no Storage** e cria um **job**.
- O job entra numa **fila** e serÃ¡ processado **em segundo plano**.
- VocÃª acompanha a evoluÃ§Ã£o (linhas lidas, inseridas, erros) atÃ© finalizar.
- Se ocorrer falha, o job fica com status **ERRO**, e Ã© possÃ­vel ver detalhes.

---

## VisÃ£o tÃ©cnica (arquitetura)

### Componentes principais

- **Controllers (upload)**:
  - SIA: `POST /api/v1/sia/import/upload`
  - SIGTAP: `POST /api/v1/sigtap/import/upload` (**requer `file` + `layoutFile`**)
  - CID10: `POST /api/v1/cid10/import/upload`
  - Comportamento: validaÃ§Ã£o leve + upload para storage + criaÃ§Ã£o de job + retorna **202 Accepted**

- **Storage**
  - ServiÃ§o: `SupabaseStorageService`
  - Upload streaming (sem carregar arquivo inteiro em memÃ³ria)

- **Fila (DB)**
  - Tabela: `public.import_job`
  - Estados principais: `ENFILEIRADO`, `PROCESSANDO`, `CONCLUIDO`, `ERRO`
  - Controle de concorrÃªncia: global e por tenant (`import.job.max-concurrent-global`, `import.job.max-concurrent-per-tenant`)

- **Scheduler**
  - Classe: `ImportJobScheduler`
  - EstratÃ©gia: `SELECT ... FOR UPDATE SKIP LOCKED` para pegar jobs sem disputa
  - Heartbeat: detecta jobs travados e re-enfileira com backoff

- **Workers**
  - SIA-PA: `SiaPaImportJobWorker`
  - SIGTAP: `SigtapImportJobWorker`
  - CID10: `Cid10ImportJobWorker`
  - Processamento: stream -> parse -> batch -> transaÃ§Ã£o curta -> checkpoint/progresso

---

## Fluxos (Mermaid)

### 1) Upload e criaÃ§Ã£o do job

```mermaid
sequenceDiagram
  autonumber
  participant UI as Frontend (Angular)
  participant API as API (Spring Boot)
  participant ST as Supabase Storage
  participant DB as Postgres (Supabase)

  UI->>API: POST /v1/*/import/upload (multipart)
  API->>ST: uploadStream(bucket, objectPath)
  API->>DB: INSERT import_job (status=PAUSADO)
  API->>DB: UPDATE import_job (status=ENFILEIRADO, storagePath, checksum, payload_json)
  API-->>UI: 202 Accepted { jobId, statusUrl }
```

### 2) Scheduler e execuÃ§Ã£o em background

```mermaid
flowchart TD
  A[Scheduler (ImportJobScheduler)] -->|FOR UPDATE SKIP LOCKED| B[Seleciona jobs ENFILEIRADO]
  B --> C{Limite global/per-tenant ok?}
  C -- NÃ£o --> A
  C -- Sim --> D[Marca job PROCESSANDO + locked_by + started_at]
  D --> E[Submit no Executor dedicado (importJobExecutor)]
  E --> F[ImportJobProcessor]
  F --> G[Worker (SIA/SIGTAP/CID10)]
```

### 3) Worker (stream + batch + checkpoint)

```mermaid
flowchart TD
  A[Worker] --> B[downloadStream(Storage)]
  B --> C[BufferedReader: linha a linha]
  C --> D[Parse + validaÃ§Ã£o]
  D --> E[Acumula batch (N linhas)]
  E --> F[TransaÃ§Ã£o curta: saveAll + flush/clear]
  F --> G[Atualiza progresso + checkpoint + heartbeat]
  G --> C
  C -->|EOF| H[Finaliza job: CONCLUIDO/ERRO]
```

### 4) Acompanhamento pelo frontend (polling)

```mermaid
sequenceDiagram
  autonumber
  participant UI as Frontend (Angular)
  participant API as API

  UI->>API: GET /v1/import-jobs/{jobId}/status
  API-->>UI: { status, linhasLidas, linhasInseridas, linhasErro, ... }
  UI->>UI: Atualiza barra de progresso + logs
  UI->>API: (repetir a cada 2-5s atÃ© status final)
```

---

## Endpoints (contrato para o frontend)

### Upload (retorna 202)

- **SIA-PA**
  - `POST /api/v1/sia/import/upload`
  - form-data:
    - `file`: CSV
    - `competenciaAno`: `YYYY`
    - `competenciaMes`: `MM`
    - `uf`: `MG`, `SP`, etc.

- **SIGTAP** (arquivo + layout)
  - `POST /api/v1/sigtap/import/upload`
  - form-data:
    - `file`: `tb_*.txt` (dados)
    - `layoutFile`: `tb_*_layout.txt` (layout)
    - `competencia`: `AAAAMM`

- **CID10**
  - `POST /api/v1/cid10/import/upload`
  - form-data:
    - `file`: CSV (ex: `CID-10-CATEGORIAS.CSV`)
    - `competencia`: `AAAAMM`

### Status e erros (polling)

- `GET /api/v1/import-jobs/{jobId}/status`
- `GET /api/v1/import-jobs/{jobId}/errors?page=0&size=50`
- `GET /api/v1/import-jobs/{jobId}/errors/count-by-code`

---

## RecomendaÃ§Ã£o de UX no Angular

- **Upload**:
  - Enviar multipart e, ao receber `202 + jobId`, navegar para uma tela de progresso.

- **Polling**:
  - Intervalo recomendado: **2sâ€“5s**
  - Parar polling quando status âˆˆ {`CONCLUIDO`, `ERRO`, `CANCELADO`}

- **ExperiÃªncia**:
  - Mostrar `linhasInseridas`, `linhasErro` e `durationMs`
  - Link/aba para listar erros paginados do job

---

## ObservaÃ§Ãµes importantes

- Os endpoints legados sÃ­ncronos (`/import/{...}`) foram marcados como **deprecated** e retornam **410** para impedir processamento pesado no ciclo HTTP.
- O SIGTAP exige layout; o job armazena o `layoutPath` em `import_job.payload_json`.
- O processamento Ã© feito em batches com transaÃ§Ãµes curtas para respeitar limites do pool de conexÃµes (Supabase).

# Sistema de ImportaÃ§Ã£o de Arquivos - Jobs AssÃ­ncronos

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral (NÃ£o TÃ©cnica)](#visÃ£o-geral-nÃ£o-tÃ©cnica)
2. [Arquitetura TÃ©cnica](#arquitetura-tÃ©cnica)
3. [Fluxos Principais](#fluxos-principais)
4. [Como Usar a API](#como-usar-a-api)
5. [Estados e Status dos Jobs](#estados-e-status-dos-jobs)
6. [Processamento e Checkpoint](#processamento-e-checkpoint)
7. [Tratamento de Erros](#tratamento-de-erros)
8. [ConfiguraÃ§Ãµes](#configuraÃ§Ãµes)

---

## ğŸ¯ VisÃ£o Geral (NÃ£o TÃ©cnica)

### O que Ã© este sistema?

Este sistema permite fazer upload de arquivos CSV grandes (milhÃµes de linhas) e processÃ¡-los de forma assÃ­ncrona, sem travar a interface do usuÃ¡rio.

### Como funciona?

Imagine que vocÃª precisa importar um arquivo gigante de dados de saÃºde:

1. **VocÃª faz upload do arquivo** â†’ O sistema recebe o arquivo e salva no armazenamento (Supabase Storage)
2. **VocÃª recebe uma resposta imediata** â†’ "Arquivo recebido! ID do job: xyz123"
3. **O sistema processa em segundo plano** â†’ Enquanto isso, vocÃª pode fazer outras coisas
4. **VocÃª acompanha o progresso** â†’ Consulta o status periodicamente (ex: "Processando... 45% concluÃ­do")
5. **VocÃª recebe o resultado** â†’ "Processamento concluÃ­do! 1.234.567 registros importados"

### Por que Ã© importante?

- **NÃ£o trava a interface**: VocÃª nÃ£o precisa esperar horas com a tela congelada
- **Pode processar arquivos gigantes**: MilhÃµes de linhas sÃ£o processadas em lotes pequenos
- **ConfianÃ§a**: Se algo der errado, o sistema tenta novamente automaticamente
- **Rastreabilidade**: VocÃª sempre sabe o que estÃ¡ acontecendo e pode ver erros especÃ­ficos

---

## ğŸ—ï¸ Arquitetura TÃ©cnica

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend       â”‚
â”‚  (Angular)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/REST
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spring Boot API                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Upload Controller            â”‚  â”‚
â”‚  â”‚  - Recebe arquivo (Multipart) â”‚  â”‚
â”‚  â”‚  - Salva no Storage           â”‚  â”‚
â”‚  â”‚  - Cria job ENFILEIRADO       â”‚  â”‚
â”‚  â”‚  - Retorna 202 Accepted       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Query Controller             â”‚  â”‚
â”‚  â”‚  - GET /import-jobs/{id}      â”‚  â”‚
â”‚  â”‚  - GET /import-jobs/{id}/statusâ”‚ â”‚
â”‚  â”‚  - GET /import-jobs/{id}/errosâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Scheduler (@Scheduled)       â”‚  â”‚
â”‚  â”‚  - Verifica fila a cada 5s    â”‚  â”‚
â”‚  â”‚  - Pega jobs pendentes        â”‚  â”‚
â”‚  â”‚  - Respeita limites           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Executor Dedicado            â”‚  â”‚
â”‚  â”‚  - Thread pool isolado        â”‚  â”‚
â”‚  â”‚  - NÃ£o usa threads do Tomcat  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Worker de Processamento      â”‚  â”‚
â”‚  â”‚  - LÃª arquivo via stream      â”‚  â”‚
â”‚  â”‚  - Processa linha a linha     â”‚  â”‚
â”‚  â”‚  - Salva em batches           â”‚  â”‚
â”‚  â”‚  - Atualiza progresso         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase       â”‚  â”‚  PostgreSQL  â”‚  â”‚  PostgreSQL     â”‚
â”‚  Storage        â”‚  â”‚  (Jobs)      â”‚  â”‚  (Dados)        â”‚
â”‚  - Arquivos     â”‚  â”‚  - Fila      â”‚  â”‚  - SIA_PA       â”‚
â”‚                 â”‚  â”‚  - Status    â”‚  â”‚  - SIGTAP       â”‚
â”‚                 â”‚  â”‚  - Progresso â”‚  â”‚  - CID10        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados Completo

```mermaid
graph TB
    subgraph Frontend[Frontend Angular]
        UI[Interface do UsuÃ¡rio]
        Poll[Polling de Status]
    end
    
    subgraph API[Spring Boot API]
        UploadCtrl[Upload Controller]
        QueryCtrl[Query Controller]
        Scheduler[Scheduler]
        Executor[Executor Dedicado]
        Worker[Worker de Processamento]
    end
    
    subgraph Storage[Supabase Storage]
        Files[Arquivos CSV]
    end
    
    subgraph DB[PostgreSQL]
        Jobs[import_job<br/>Fila + Status]
        Errors[import_job_error<br/>Erros Detalhados]
        Data[sia_pa / sigtap / cid10<br/>Dados Importados]
    end
    
    UI -->|POST /upload<br/>MultipartFile| UploadCtrl
    UploadCtrl -->|Salva arquivo| Files
    UploadCtrl -->|Cria registro| Jobs
    UploadCtrl -->|202 Accepted<br/>jobId| UI
    
    UI -->|GET /import-jobs/id/status<br/>Polling a cada 2-5s| QueryCtrl
    QueryCtrl -->|Consulta| Jobs
    QueryCtrl -->|Retorna status| UI
    
    Scheduler -->|SELECT FOR UPDATE<br/>SKIP LOCKED| Jobs
    Scheduler -->|Respeita limites<br/>global e por tenant| Executor
    Executor -->|Processa assÃ­ncrono| Worker
    
    Worker -->|Stream do arquivo| Files
    Worker -->|LÃª linha a linha| Worker
    Worker -->|Batch insert| Data
    Worker -->|Atualiza progresso| Jobs
    Worker -->|Registra erros| Errors
    
    style UploadCtrl fill:#e1f5ff
    style Worker fill:#fff4e1
    style Scheduler fill:#f0e1ff
    style Jobs fill:#e8f5e9
```

---

## ğŸ”„ Fluxos Principais

### 1. Fluxo de Upload de Arquivo

```mermaid
sequenceDiagram
    participant U as UsuÃ¡rio (Frontend)
    participant API as Upload Controller
    participant Storage as Supabase Storage
    participant DB as PostgreSQL (import_job)
    
    U->>API: POST /v1/import-jobs/upload<br/>(MultipartFile + metadados)
    
    Note over API: Valida arquivo:<br/>- Tamanho mÃ¡ximo<br/>- Tipo/format<br/>- CompetÃªncia/UF
    
    API->>Storage: Upload via stream<br/>(InputStream â†’ HTTP POST)
    Storage-->>API: Arquivo salvo<br/>(path/bucket)
    
    API->>DB: INSERT INTO import_job<br/>(status=ENFILEIRADO)
    DB-->>API: Job criado (UUID)
    
    API-->>U: 202 Accepted<br/>{jobId, status: "ENFILEIRADO"}
    
    Note over U: Upload concluÃ­do!<br/>UsuÃ¡rio pode fazer outras coisas
```

### 2. Fluxo de Processamento (Scheduler + Worker)

```mermaid
sequenceDiagram
    participant S as Scheduler<br/>(@Scheduled cada 5s)
    participant DB as PostgreSQL (import_job)
    participant E as Executor<br/>(Thread Pool)
    participant W as Worker<br/>(Processamento)
    participant Storage as Supabase Storage
    participant DataDB as PostgreSQL<br/>(dados finais)
    
    loop A cada 5 segundos
        S->>DB: SELECT * FROM import_job<br/>WHERE status='ENFILEIRADO'<br/>AND next_run_at <= NOW()<br/>ORDER BY priority DESC<br/>FOR UPDATE SKIP LOCKED<br/>LIMIT 1
        
        alt Job encontrado e dentro dos limites
            DB-->>S: Job pendente
            
            S->>DB: UPDATE import_job<br/>SET status='PROCESSANDO',<br/>locked_at=NOW(),<br/>started_at=NOW()
            
            S->>E: Executar processamento<br/>(assÃ­ncrono)
            
            Note over E: Thread dedicada<br/>(nÃ£o usa Tomcat)
            
            E->>W: processarJob(jobId)
            
            W->>Storage: Abrir stream do arquivo<br/>(InputStream)
            Storage-->>W: InputStream do CSV
            
            loop Para cada linha do arquivo
                W->>W: Ler linha via BufferedReader
                W->>W: Parse CSV (comma, quotes)
                W->>W: Validar campos obrigatÃ³rios
                W->>W: Converter para entidade
                W->>W: Adicionar ao batch
                
                alt Batch completo (1000 linhas)
                    W->>DataDB: BEGIN TRANSACTION
                    W->>DataDB: INSERT batch (saveAll)
                    W->>DataDB: COMMIT
                    W->>DB: UPDATE import_job<br/>SET linhas_processadas=+1000,<br/>checkpoint_linha=linha_atual,<br/>heartbeat_at=NOW()
                    W->>W: Limpar batch (flush/clear)
                end
            end
            
            W->>DB: UPDATE import_job<br/>SET status='CONCLUIDO',<br/>finished_at=NOW(),<br/>duration_ms=calculo
            
        else Nenhum job ou limite atingido
            DB-->>S: Nenhum job disponÃ­vel
            Note over S: Aguarda prÃ³ximo ciclo
        end
    end
```

### 3. Fluxo de Acompanhamento pelo Frontend

```mermaid
sequenceDiagram
    participant U as UsuÃ¡rio
    participant F as Frontend Angular
    participant API as Query Controller
    participant DB as PostgreSQL
    
    Note over U: Upload concluÃ­do<br/>Job ID recebido
    
    loop Polling (a cada 2-5 segundos)
        F->>API: GET /v1/import-jobs/{jobId}/status
        API->>DB: SELECT status, progresso<br/>FROM import_job WHERE id=?
        DB-->>API: {status, linhasProcessadas, ...}
        API-->>F: {status: "PROCESSANDO",<br/>linhasProcessadas: 45000,<br/>percentualEstimado: 45%}
        
        F->>F: Atualizar UI<br/>(barra de progresso)
        F-->>U: Exibir: "Processando... 45%"
        
        alt Status = CONCLUIDO
            F->>API: GET /v1/import-jobs/{jobId}/erros?page=0
            API->>DB: SELECT * FROM import_job_error<br/>WHERE job_id=? ORDER BY linha
            DB-->>API: Lista de erros (paginada)
            API-->>F: {erros: [...], total: 10}
            
            F-->>U: "ConcluÃ­do!<br/>1.234.567 registros importados<br/>10 erros encontrados"
            Note over F: Parar polling
        else Status = ERRO
            F-->>U: "Erro no processamento<br/>Ver detalhes"
            Note over F: Parar polling
        end
    end
```

### 4. Fluxo de Estados do Job

```mermaid
stateDiagram-v2
    [*] --> ENFILEIRADO: Upload concluÃ­do<br/>Job criado
    
    ENFILEIRADO --> PROCESSANDO: Scheduler pega job<br/>(respeitando limites)
    
    PROCESSANDO --> PROCESSANDO: Atualiza progresso<br/>a cada batch<br/>(heartbeat)
    
    PROCESSANDO --> CONCLUIDO: Arquivo processado<br/>com sucesso<br/>(EOF alcanÃ§ado)
    
    PROCESSANDO --> ERRO: Erro fatal<br/>ou max_attempts<br/>excedido
    
    PROCESSANDO --> ENFILEIRADO: Heartbeat expirado<br/>(job travado)<br/>Retry com backoff
    
    ERRO --> [*]: Fim (nÃ£o retenta)
    CONCLUIDO --> [*]: Fim (sucesso)
    
    note right of ENFILEIRADO
        next_run_at <= NOW()
        priority considerado
        Limites verificados
    end note
    
    note right of PROCESSANDO
        heartbeat_at atualizado
        a cada batch
        checkpoint_linha salvo
    end note
```

### 5. Fluxo de RecuperaÃ§Ã£o (Checkpoint)

```mermaid
sequenceDiagram
    participant W as Worker
    participant DB as import_job
    participant Storage as Supabase Storage
    participant DataDB as Dados Finais
    
    Note over W: Job em PROCESSANDO<br/>com checkpoint_linha = 45000
    
    W->>DB: SELECT checkpoint_linha<br/>FROM import_job WHERE id=?
    DB-->>W: checkpoint_linha = 45000
    
    W->>Storage: Abrir stream do arquivo
    Storage-->>W: InputStream
    
    loop Pular linhas atÃ© checkpoint
        W->>W: reader.readLine()<br/>(descartar linha)
        Note over W: Linha 1...45000<br/>descartadas
    end
    
    Note over W: ComeÃ§ar processamento<br/>a partir da linha 45001
    
    loop Linhas restantes
        W->>W: Processar linha
        W->>DataDB: Batch insert
        
        W->>DB: UPDATE checkpoint_linha<br/>= linha_atual
    end
    
    W->>DB: UPDATE status='CONCLUIDO'
```

### 6. Fluxo de Controle de ConcorrÃªncia

```mermaid
graph TB
    subgraph Scheduler[Scheduler - Ciclo de VerificaÃ§Ã£o]
        Check[Verificar fila]
        CountGlobal[Contar jobs PROCESSANDO<br/>globalmente]
        CheckLimits{Global < max<br/>concurrent?}
        CountTenant[Agrupar por tenant<br/>contar PROCESSANDO por tenant}
        CheckTenantLimits{Tenant < max<br/>per tenant?}
        PickJob[SELECT FOR UPDATE<br/>SKIP LOCKED<br/>ORDER BY priority]
        Execute[Enviar para Executor]
    end
    
    Check --> CountGlobal
    CountGlobal --> CheckLimits
    CheckLimits -->|Sim| CountTenant
    CheckLimits -->|NÃ£o| Wait[Aguardar prÃ³ximo ciclo]
    CountTenant --> CheckTenantLimits
    CheckTenantLimits -->|Sim| PickJob
    CheckTenantLimits -->|NÃ£o| Wait
    PickJob -->|Job encontrado| Execute
    PickJob -->|Nenhum job| Wait
    Execute --> Wait
    Wait --> Check
    
    style CheckLimits fill:#fff4e1
    style CheckTenantLimits fill:#fff4e1
    style PickJob fill:#e1f5ff
```

---

## ğŸ“¡ Como Usar a API

### 1. Upload de Arquivo

**Endpoint:** `POST /v1/import-jobs/upload`

**Request:**
```http
POST /api/v1/import-jobs/upload
Content-Type: multipart/form-data

{
  "file": <arquivo.csv>,
  "tipo": "SIA_PA",
  "competenciaAno": "2025",
  "competenciaMes": "01",
  "uf": "MG"
}
```

**Response (202 Accepted):**
```json
{
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "status": "ENFILEIRADO",
  "mensagem": "Upload concluÃ­do. Processamento iniciarÃ¡ em breve.",
  "originalFilename": "PAMG2501a.csv",
  "sizeBytes": 104857600,
  "statusUrl": "/api/v1/import-jobs/550e8400-e29b-41d4-a716-446655440000/status",
  "errosUrl": "/api/v1/import-jobs/550e8400-e29b-41d4-a716-446655440000/erros"
}
```

### 2. Consultar Status (Polling)

**Endpoint:** `GET /v1/import-jobs/{jobId}/status`

**Response:**
```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "PROCESSANDO",
  "linhasLidas": 45000,
  "linhasProcessadas": 45000,
  "linhasInseridas": 44950,
  "linhasErro": 50,
  "percentualEstimado": 45.0,
  "startedAt": "2025-01-15T10:30:00Z",
  "finishedAt": null,
  "durationMs": null,
  "errorSummary": null,
  "totalErros": 50
}
```

**Estados possÃ­veis:**
- `ENFILEIRADO`: Aguardando processamento
- `PROCESSANDO`: Em processamento
- `CONCLUIDO`: Processamento concluÃ­do com sucesso
- `ERRO`: Processamento falhou
- `CANCELADO`: Cancelado pelo usuÃ¡rio
- `PAUSADO`: Pausado (futuro)

### 3. Listar Jobs

**Endpoint:** `GET /v1/import-jobs?page=0&size=20&sortBy=createdAt&sortDir=DESC`

**Response:**
```json
{
  "content": [
    {
      "id": "...",
      "tipo": "SIA_PA",
      "status": "CONCLUIDO",
      "originalFilename": "PAMG2501a.csv",
      "linhasProcessadas": 100000,
      "createdAt": "2025-01-15T10:00:00Z",
      ...
    }
  ],
  "totalElements": 15,
  "totalPages": 1,
  "number": 0,
  "size": 20
}
```

### 4. Listar Erros de um Job

**Endpoint:** `GET /v1/import-jobs/{jobId}/erros?page=0&size=50`

**Response:**
```json
{
  "content": [
    {
      "id": "...",
      "linha": 123,
      "codigoErro": "VALIDATION_ERROR",
      "mensagem": "Campo obrigatÃ³rio ausente: PA_PROC_ID",
      "rawLinePreview": "MG,2501,1234567,..."
    }
  ],
  "totalElements": 50,
  "totalPages": 5
}
```

---

## ğŸ“Š Estados e Status dos Jobs

### Diagrama de Estados Detalhado

```mermaid
stateDiagram-v2
    [*] --> ENFILEIRADO: Upload + criaÃ§Ã£o do job
    
    ENFILEIRADO --> PROCESSANDO: Scheduler seleciona job<br/>(lock transacional)
    
    PROCESSANDO --> PROCESSANDO: Batch processado<br/>(atualiza progresso)
    
    PROCESSANDO --> CONCLUIDO: EOF alcanÃ§ado<br/>Batch final commitado
    
    PROCESSANDO --> ERRO: Erro fatal<br/>(ex: arquivo invÃ¡lido)
    
    PROCESSANDO --> ENFILEIRADO: Heartbeat expirado<br/>(job travado)<br/>attempts++, backoff
    
    ENFILEIRADO --> ENFILEIRADO: Max attempts excedido<br/>â†’ ERRO
    
    CONCLUIDO --> [*]
    ERRO --> [*]
    
    note right of ENFILEIRADO
        Campos:
        - next_run_at
        - priority
        - attempts
        - max_attempts
    end note
    
    note right of PROCESSANDO
        Campos atualizados:
        - heartbeat_at (a cada batch)
        - checkpoint_linha
        - linhas_processadas
        - linhas_inseridas
        - linhas_erro
    end note
```

### Campos de Progresso

| Campo | DescriÃ§Ã£o | Quando Ã© atualizado |
|-------|-----------|---------------------|
| `linhasLidas` | Total de linhas lidas do arquivo | A cada linha |
| `linhasProcessadas` | Linhas que passaram pela validaÃ§Ã£o | ApÃ³s validaÃ§Ã£o |
| `linhasInseridas` | Linhas inseridas no banco com sucesso | ApÃ³s commit do batch |
| `linhasErro` | Linhas com erro | Quando erro Ã© registrado |
| `percentualEstimado` | Porcentagem estimada (se calculÃ¡vel) | A cada atualizaÃ§Ã£o |
| `checkpointLinha` | Ãšltima linha processada com sucesso | A cada batch commitado |
| `heartbeatAt` | Ãšltima atualizaÃ§Ã£o (detecta jobs travados) | A cada batch |

---

## âš™ï¸ Processamento e Checkpoint

### EstratÃ©gia de Processamento em Batches

```mermaid
graph LR
    A[Arquivo CSV<br/>1.000.000 linhas] --> B[Stream Reader<br/>BufferedReader]
    B --> C[Linha 1]
    C --> D[Parse CSV]
    D --> E[Validar]
    E --> F[Converter Entidade]
    F --> G[Adicionar ao Batch]
    
    G --> H{Batch completo?<br/>1000 linhas}
    H -->|NÃ£o| C
    H -->|Sim| I[TransaÃ§Ã£o Curta]
    
    I --> J[INSERT batch<br/>saveAll]
    J --> K[COMMIT]
    K --> L[flush + clear<br/>Limpar contexto JPA]
    L --> M[UPDATE import_job<br/>checkpoint + progresso]
    M --> N{Batch final?}
    N -->|NÃ£o| C
    N -->|Sim| O[CONCLUIDO]
    
    style I fill:#fff4e1
    style K fill:#e8f5e9
    style L fill:#e1f5ff
```

### TransaÃ§Ãµes Curtas vs Longas

**âŒ ERRADO (TransaÃ§Ã£o Longa):**
```java
@Transactional(timeout = 7200) // 2 horas - PROBLEMA!
public void processarArquivo() {
    // Processa milhÃµes de linhas em UMA transaÃ§Ã£o
    // ConexÃ£o ocupada por horas
    // Risco de timeout, deadlock, pool esgotado
}
```

**âœ… CORRETO (TransaÃ§Ãµes Curtas):**
```java
// Sem @Transactional no mÃ©todo principal
public void processarArquivo() {
    for (Batch batch : batches) {
        processarBatch(batch); // TransaÃ§Ã£o curta por batch
    }
}

@Transactional // TransaÃ§Ã£o curta (segundos)
private void processarBatch(Batch batch) {
    repository.saveAll(batch);
    // COMMIT imediato
    // ConexÃ£o liberada
}
```

### Checkpoint e RecuperaÃ§Ã£o

**Como funciona:**

1. **Durante processamento:** A cada batch commitado, salva `checkpoint_linha = linha_atual`
2. **Em caso de falha:** Job volta para `ENFILEIRADO` com `attempts++`
3. **Ao retomar:** Worker lÃª `checkpoint_linha` e pula linhas atÃ© chegar no ponto de retomada
4. **Vantagem:** NÃ£o precisa reprocessar linhas jÃ¡ importadas

**Exemplo:**

```
Job inicia: checkpoint_linha = 0
Processa linhas 1-1000: checkpoint_linha = 1000 âœ…
Processa linhas 1001-2000: checkpoint_linha = 2000 âœ…
CRASH! (servidor caiu)

Job reinicia: checkpoint_linha = 2000
Pula linhas 1-2000 (jÃ¡ processadas)
Continua da linha 2001 âœ…
```

---

## ğŸš¨ Tratamento de Erros

### Tipos de Erros

1. **Erros de Linha (nÃ£o bloqueiam):**
   - Campo obrigatÃ³rio ausente
   - Formato invÃ¡lido
   - Valor fora do esperado
   - **AÃ§Ã£o:** Registra em `import_job_error`, continua processamento

2. **Erros de Batch (bloqueiam batch atual):**
   - Erro de validaÃ§Ã£o JPA
   - Constraint violation
   - **AÃ§Ã£o:** Rollback do batch, registra erro, continua prÃ³ximo batch

3. **Erros Fatais (bloqueiam job):**
   - Arquivo nÃ£o encontrado
   - Erro de conexÃ£o com Storage
   - Erro de conexÃ£o com banco
   - **AÃ§Ã£o:** Marca job como `ERRO`, nÃ£o retenta

### Fluxo de Erro e Retry

```mermaid
graph TB
    A[Erro durante processamento] --> B{Tipo de erro?}
    
    B -->|Erro de linha| C[Registra em<br/>import_job_error]
    C --> D[Continua processamento]
    
    B -->|Erro de batch| E[ROLLBACK batch]
    E --> F[Registra erro resumido]
    F --> G[Continua prÃ³ximo batch]
    
    B -->|Erro fatal| H[Marca job como ERRO]
    H --> I[Atualiza error_summary]
    I --> J[Job finalizado<br/>Sem retry]
    
    B -->|Job travado<br/>heartbeat expirado| K[Reset para ENFILEIRADO]
    K --> L{attempts < max_attempts?}
    L -->|Sim| M[Incrementa attempts<br/>next_run_at = now + backoff]
    L -->|NÃ£o| H
    M --> N[Aguarda prÃ³ximo ciclo<br/>do scheduler]
    
    style C fill:#fff4e1
    style E fill:#ffe1e1
    style H fill:#ffcccc
```

---

## ğŸ”§ ConfiguraÃ§Ãµes

### application.properties

```properties
# ============================================
# Upload de Arquivos
# ============================================
spring.servlet.multipart.max-file-size=500MB
spring.servlet.multipart.max-request-size=500MB
spring.servlet.multipart.file-size-threshold=1MB
server.tomcat.max-swallow-size=-1
server.tomcat.connection-timeout=60000

# ============================================
# Jobs de ImportaÃ§Ã£o
# ============================================
# ConcorrÃªncia global (nÃºmero mÃ¡ximo de jobs processando simultaneamente)
import.job.max-concurrent-global=1

# ConcorrÃªncia por tenant
import.job.max-concurrent-per-tenant=1

# Intervalo do scheduler (segundos)
import.job.scheduler-interval-seconds=5

# Timeout de heartbeat (segundos) - jobs sem heartbeat por mais tempo sÃ£o considerados travados
import.job.heartbeat-timeout-seconds=300

# Prioridades padrÃ£o por tipo (maior nÃºmero = maior prioridade)
import.job.priority.sia-pa=100
import.job.priority.sigtap=50
import.job.priority.cid10=10

# Tamanho de batch para processamento (linhas por batch)
import.job.batch-size=1000

# Intervalo para atualizar progresso (nÃºmero de batches entre atualizaÃ§Ãµes)
import.job.progress-update-interval-batches=10
```

### Dockerfile (Heap de MemÃ³ria)

```dockerfile
ENTRYPOINT ["java", "-Xms256m", "-Xmx1g", "-XX:+UseG1GC", "-XX:MaxGCPauseMillis=200", "-jar", "app.jar"]
```

**ExplicaÃ§Ã£o:**
- `-Xms256m`: Heap inicial de 256MB
- `-Xmx1g`: Heap mÃ¡ximo de 1GB (ajustar conforme recursos)
- `-XX:+UseG1GC`: Garbage Collector G1 (melhor para server-side)
- `-XX:MaxGCPauseMillis=200`: Tenta manter pausas do GC abaixo de 200ms

---

## ğŸ“ˆ MÃ©tricas e Observabilidade

### MÃ©tricas Expostas (Actuator)

- `import.jobs.status.{status}.count`: Contagem de jobs por status
- `import.jobs.processing.throughput`: Linhas processadas por segundo
- `import.jobs.processing.duration`: DuraÃ§Ã£o mÃ©dia de processamento
- `import.jobs.errors.count`: Total de erros registrados
- `import.jobs.queue.wait-time`: Tempo mÃ©dio de espera na fila

### Logs Estruturados

```
INFO  - Job iniciado: jobId=xxx, tipo=SIA_PA, tenant=yyy
INFO  - Batch processado: jobId=xxx, batch=5, linhas=5000, tempo=1234ms
WARN  - Erros no batch: jobId=xxx, batch=5, erros=3
INFO  - Progresso atualizado: jobId=xxx, linhasProcessadas=50000, percentual=50%
INFO  - Job concluÃ­do: jobId=xxx, linhasInseridas=100000, duracao=3600000ms
ERROR - Erro fatal: jobId=xxx, erro=Arquivo nÃ£o encontrado
```

---

## ğŸ“ Conceitos TÃ©cnicos Importantes

### Por que usar fila no banco?

- âœ… **Auditoria completa:** HistÃ³rico de todos os jobs
- âœ… **Rastreabilidade:** Sempre sabe quem iniciou, quando, status atual
- âœ… **Sem dependÃªncia externa:** NÃ£o precisa de Redis/RabbitMQ
- âœ… **Transacional:** Lock `FOR UPDATE SKIP LOCKED` evita duplicidade
- âœ… **Query flexÃ­vel:** FÃ¡cil filtrar por tenant, tipo, status

### Por que transaÃ§Ãµes curtas?

- âœ… **Pool de conexÃµes limitado:** ConexÃµes ficam ocupadas por segundos, nÃ£o horas
- âœ… **Evita timeouts:** `max-lifetime` de conexÃµes nÃ£o Ã© ultrapassado
- âœ… **Melhor performance:** Menos lock time no banco
- âœ… **Resiliente:** Falha em um batch nÃ£o invalida os anteriores

### Por que executor dedicado?

- âœ… **Isolamento:** NÃ£o compete com threads do Tomcat (servidor web)
- âœ… **Controle:** Tamanho do pool configurÃ¡vel
- âœ… **PriorizaÃ§Ã£o:** Fila prÃ³pria para jobs pesados
- âœ… **Backpressure:** Rejeita novos jobs se fila cheia

### Por que checkpoint?

- âœ… **RecuperaÃ§Ã£o:** Retoma de onde parou apÃ³s falha
- âœ… **EficiÃªncia:** NÃ£o reprocessa linhas jÃ¡ importadas
- âœ… **Confiabilidade:** Jobs longos nÃ£o sÃ£o perdidos completamente

---

## ğŸ“ Resumo Executivo

### Fluxo Simplificado

```
1. Upload â†’ Storage â†’ Job criado â†’ 202 Accepted
2. Scheduler verifica fila (a cada 5s)
3. Worker processa: stream â†’ parse â†’ batch â†’ insert â†’ checkpoint
4. Frontend faz polling de status
5. Job concluÃ­do: usuÃ¡rio recebe resultado
```

### CaracterÃ­sticas Principais

- âœ… **AssÃ­ncrono:** Upload retorna imediatamente
- âœ… **EscalÃ¡vel:** Processa milhÃµes de linhas em batches
- âœ… **Resiliente:** Checkpoint + retry automÃ¡tico
- âœ… **RastreÃ¡vel:** Status, progresso e erros detalhados
- âœ… **Isolado:** NÃ£o impacta performance da API
- âœ… **Multi-tenant:** Limites e isolamento por tenant

---

**Ãšltima atualizaÃ§Ã£o:** Janeiro 2025
**VersÃ£o:** 1.0

